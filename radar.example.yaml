# Radar Configuration
# Copy this file to ~/.config/radar/radar.yaml for global config
# or keep in project directory for project-specific config

llm:
  # LLM provider: "ollama" (default) or "openai" (OpenAI-compatible API)
  provider: ollama
  # Ollama API base URL
  base_url: "http://localhost:11434"
  # Model to use for chat
  model: "qwen3:latest"
  # Fallback model for rate limit errors (e.g., cloud models with quotas)
  # fallback_model: "qwen3:latest"

notifications:
  # ntfy.sh server URL
  url: "https://ntfy.sh"
  # Your ntfy topic (required for notifications to work)
  topic: ""

tools:
  # Maximum file size for read_file tool (bytes)
  max_file_size: 102400
  # Timeout for exec tool (seconds)
  exec_timeout: 30

# Retry with exponential backoff for transient errors
# retry:
#   max_retries: 3          # Number of retries (0 = disable)
#   base_delay: 1.0         # Base delay in seconds
#   max_delay: 30.0         # Maximum delay cap
#   llm_retries: true       # Retry LLM API calls
#   embedding_retries: true # Retry embedding API calls
#   url_monitor_retries: true # Retry URL monitor fetches

# Maximum number of tool call iterations before stopping
max_tool_iterations: 10

# File watchers - monitor directories for changes
# watch_paths:
#   - path: ~/Downloads
#     patterns: ["*.pdf"]
#     description: "Downloads"
#     action: "Summarize this PDF and notify me via ntfy"

# Custom system prompt (optional)
# Use {current_time} placeholder for current timestamp
# system_prompt: |
#   You are a helpful assistant...
